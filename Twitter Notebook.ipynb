{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Authorize with Twitter"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Retrieve tweets related to search and store for analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# https://pypi.python.org/pypi/twitter\n",
      "# https://dev.twitter.com/docs/api/1.1\n",
      "# \n",
      "\n",
      "\n",
      "from twitter import *\n",
      "import json\n",
      "\n",
      "OAUTH_TOKEN = '29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo'\n",
      "OAUTH_SECRET = 'dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds'\n",
      "CONSUMER_KEY = 'xLNSlDfsYDWoVDVlnUewog'\n",
      "CONSUMER_SECRET = 'cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc'\n",
      "\n",
      "# see \"Authentication\" section below for tokens and keys\n",
      "t = Twitter(\n",
      "            auth=OAuth(OAUTH_TOKEN, OAUTH_SECRET,\n",
      "                       CONSUMER_KEY, CONSUMER_SECRET)\n",
      "           )\n",
      "\n",
      "# Get your \"home\" timeline\n",
      "#t.statuses.home_timeline()\n",
      "query = \"#bigdata\"\n",
      "myCount = 1000\n",
      "myStart = 1\n",
      "\n",
      "# twitter search\n",
      "r = t.search.tweets(q=query,count=myCount)\n",
      "tweets = json.loads(json.dumps(r['statuses'], indent=1))\n",
      "my_max_id = r['search_metadata']['max_id']\n",
      "\n",
      "#write to file for review\n",
      "tweets_data = 'C:/Projects/ipython/tweets.json'\n",
      "f = open(tweets_data, 'w')\n",
      "f.write(json.dumps(r, indent=2))\n",
      "f.close()\n",
      "\n",
      "for k in range(1,5): \n",
      "    try:\n",
      "        next_results = r['search_metadata']['next_results']\n",
      "        my_max_id = r['search_metadata']['max_id']\n",
      "        #print my_max_id\n",
      "        \n",
      "    except KeyError, e: # No more results when next_results doesn't exist\n",
      "        break\n",
      "    \n",
      "    r = t.search.tweets(q=query,count=myCount,max_id=my_max_id)\n",
      "    tweets += json.loads(json.dumps(r['statuses'], indent=1))\n",
      "\n",
      "from openpyxl import Workbook\n",
      "wb = Workbook()\n",
      "ws = wb.get_active_sheet()\n",
      "\n",
      "dest_filename = 'C:/Projects/ipython/twitter.xlsx'#r'test_book.xlsx'\n",
      "\n",
      "ws = wb.worksheets[0]\n",
      "\n",
      "ws.title = \"Twitter\"\n",
      "\n",
      "# create header\n",
      "ws.cell('A1').value = 'TweetID'\n",
      "ws.cell('B1').value = 'TweetDateTime'\n",
      "ws.cell('C1').value = 'Author_ID'\n",
      "ws.cell('D1').value = 'TweetText'\n",
      "ws.cell('E1').value = 'RetweetCount'\n",
      "ws.cell('F1').value = 'FavoriteCount'\n",
      "ws.cell('G1').value = 'UserMentions'\n",
      "ws.cell('H1').value = 'Hashtags'\n",
      "ws.cell('I1').value = 'URLs'\n",
      "ws.cell('J1').value = 'AuthorID'\n",
      "ws.cell('K1').value = 'FollowersCount'\n",
      "ws.cell('L1').value = 'FriendsCount'\n",
      "ws.cell('M1').value = 'StatusesCount'\n",
      "ws.cell('N1').value = 'UserLocation'\n",
      "ws.cell('O1').value = 'Description'\n",
      "ws.cell('P1').value = 'OriginalAuthorID'\n",
      "ws.cell('Q1').value = 'OriginalTweetText'\n",
      "ws.cell('R1').value = 'OriginalRetweetCount'\n",
      "ws.cell('S1').value = 'OriginalFavoriteCount'\n",
      "\n",
      "i = 2\n",
      "for tweet in tweets:\n",
      "    if 'id' in tweet: #'TweetID'\n",
      "        ws.cell('%s%s'%('A', i)).value = tweet['id'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('A', i)).value = \"\" \n",
      "    \n",
      "    if 'created_at' in tweet: #'TweetDateTime'\n",
      "        ws.cell('%s%s'%('B', i)).value = tweet['created_at'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('B', i)).value = \"\"        \n",
      "    \n",
      "    if 'user' in tweet: #'Author_ID'\n",
      "        ws.cell('%s%s'%('C', i)).value = tweet['user']['id'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('C', i)).value = \"\"\n",
      "        \n",
      "    if 'text' in tweet: #'TweetText'\n",
      "        ws.cell('%s%s'%('D', i)).value = tweet['text'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('D', i)).value = \"\"\n",
      "        \n",
      "    if 'retweet_count' in tweet: #'RetweetCount'\n",
      "        ws.cell('%s%s'%('E', i)).value = tweet['retweet_count'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('E', i)).value = \"\"\n",
      "        \n",
      "    if 'favorite_count' in tweet: #'FavoriteCount'\n",
      "        ws.cell('%s%s'%('F', i)).value = tweet['favorite_count'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('F', i)).value = \"\"\n",
      "        \n",
      "    if 'entities' in tweet: #'UserMentions'\n",
      "        if 'user_mentions' in tweet['entities']:\n",
      "            mentions = \"\"\n",
      "            for j in range(0,len(tweet['entities']['user_mentions'])):\n",
      "                mentions = mentions + str(tweet['entities']['user_mentions'][j]['id']) + \"|\"\n",
      "            ws.cell('%s%s'%('G', i)).value = mentions\n",
      "    else:\n",
      "        ws.cell('%s%s'%('G', i)).value = \"\"\n",
      "        \n",
      "    if 'entities' in tweet: #'Hashtags'\n",
      "        ws.cell('%s%s'%('H', i)).value = tweet['entities']['hashtags'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('H', i)).value = \"\"\n",
      "        \n",
      "    if 'entities' in tweet: #'URLs'\n",
      "        if 'urls' in tweet['entities']:\n",
      "            urls = \"\"\n",
      "            for j in range(0,len(tweet['entities']['urls'])):\n",
      "                urls = urls + tweet['entities']['urls'][j]['expanded_url'] + \"|\"\n",
      "            ws.cell('%s%s'%('I', i)).value = urls \n",
      "    else:\n",
      "        ws.cell('%s%s'%('I', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'AuthorID'\n",
      "        ws.cell('%s%s'%('J', i)).value = tweet['user']['id']\n",
      "    else:\n",
      "        ws.cell('%s%s'%('J', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'FollowersCount'\n",
      "        ws.cell('%s%s'%('K', i)).value = tweet['user']['followers_count'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('K', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'FriendsCount'\n",
      "        ws.cell('%s%s'%('L', i)).value = tweet['user']['friends_count'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('L', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'StatusesCount'\n",
      "        ws.cell('%s%s'%('M', i)).value = tweet['user']['statuses_count']\n",
      "    else:\n",
      "        ws.cell('%s%s'%('M', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'UserLocation'\n",
      "        ws.cell('%s%s'%('N', i)).value = tweet['user']['location']\n",
      "    else:\n",
      "        ws.cell('%s%s'%('N', i)).value = \"\"\n",
      "        \n",
      "    if 'user' in tweet: #'Description'\n",
      "        ws.cell('%s%s'%('O', i)).value = tweet['user']['description'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('O', i)).value = \"\"\n",
      "        \n",
      "    if 'retweeted_status' in tweet: #'OriginalAuthorID'\n",
      "        ws.cell('%s%s'%('P', i)).value = tweet['retweeted_status']['user']['id'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('P', i)).value = \"\"\n",
      "        \n",
      "    if 'retweeted_status' in tweet: #'OriginalTweetText'\n",
      "        ws.cell('%s%s'%('Q', i)).value = tweet['retweeted_status']['text']\n",
      "    else:\n",
      "        ws.cell('%s%s'%('Q', i)).value = \"\"\n",
      "        \n",
      "    if 'retweeted_status' in tweet: #'OriginalRetweetCount'\n",
      "        ws.cell('%s%s'%('R', i)).value = tweet['retweeted_status']['retweet_count']\n",
      "    else:\n",
      "        ws.cell('%s%s'%('R', i)).value = \"\"\n",
      "        \n",
      "    if 'retweeted_status' in tweet: #'Description'\n",
      "        ws.cell('%s%s'%('S', i)).value = tweet['retweeted_status']['favorite_count'] \n",
      "    else:\n",
      "        ws.cell('%s%s'%('S', i)).value = \"\"\n",
      "        \n",
      "    i += 1\n",
      "   \n",
      "wb.save(filename = dest_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Retrieve tweets and append new tweets to previously collected tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for _ in range(5): \n",
      "    try:\n",
      "        next_results = search_results['search_metadata']['next_results']\n",
      "    except KeyError, e: # No more results when next_results doesn't exist\n",
      "        break\n",
      "\n",
      "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ]) # Create a dictionary from the query string params\n",
      "    search_results = twitter_api.search.tweets(**kwargs)\n",
      "    statuses += search_results['statuses']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# https://pypi.python.org/pypi/twitter\n",
      "# https://dev.twitter.com/docs/api/1.1\n",
      "# \n",
      "\n",
      "\n",
      "from twitter import *\n",
      "import json\n",
      "\n",
      "OAUTH_TOKEN = '29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo'\n",
      "OAUTH_SECRET = 'dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds'\n",
      "CONSUMER_KEY = 'xLNSlDfsYDWoVDVlnUewog'\n",
      "CONSUMER_SECRET = 'cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc'\n",
      "\n",
      "# see \"Authentication\" section below for tokens and keys\n",
      "t = Twitter(\n",
      "            auth=OAuth(OAUTH_TOKEN, OAUTH_SECRET,\n",
      "                       CONSUMER_KEY, CONSUMER_SECRET)\n",
      "           )\n",
      "\n",
      "# Get your \"home\" timeline\n",
      "#t.statuses.home_timeline()\n",
      "\n",
      "# twitter search\n",
      "r = t.search.tweets(q=\"#bigdata\")\n",
      "tweet = json.loads(json.dumps(r, indent=1))\n",
      "\n",
      "for status in tweet['statuses']:\n",
      "    if 'text' in status:\n",
      "        print status['text'] + \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Big Pharma companies are learning lots about individuals without violating HIPAA. Concerned? #BigData #Healthcare http://t.co/RYRYuAWTvH\n",
        "\n",
        "RT @jpportron: #Orange #cloudcomputing #bigdata: nouveaux services, nouveaux enjeux comp\u00e9tences. Le CE #Orange Normandie Centre au #datacen\u2026\n",
        "\n",
        "A great talk by #IBM's Inhi Suh on #BigData myths https://t.co/CzxU1sLjxp\n",
        "\n",
        "RT @iioannoulbs: Target Confirms Millions Of Customer Credit Cards Hacked Since Black Friday http://t.co/DhKpIRcROd #privacy #bigdata #acco\u2026\n",
        "\n",
        "Emerging #Electricity Value Chain | #smartgrid #smartmeters #bigdata #ibm http://t.co/e1bmQfRBRs\n",
        "\n",
        "@Syncsort has found new life with what enterprises with legacy systems require for #Hadoop and #bigdata analytics http://t.co/rLEMEYxWDf\n",
        "\n",
        "RT @SolvingBacon: Science of Bacon intro slides available: http://t.co/t7weE8Fjt7 #data #datastrategy #Strategy #privacy  #bigdata\n",
        "\n",
        "Expectativas multimillonarias para #bigdata en los pr\u00f3ximos 4 a\u00f1os http://t.co/SwpMtpP5cp\n",
        "\n",
        "RT @varonis: [Free eBook] 10 Insights on Enterprise Data:  \n",
        "\n",
        "http://t.co/nJVtY8CHfc \n",
        "\n",
        "#bigdata #dataprotection #datagovernance http://t.co/\u2026\n",
        "\n",
        "Science of Bacon intro slides available: http://t.co/t7weE8Fjt7 #data #datastrategy #Strategy #privacy  #bigdata\n",
        "\n",
        "#BigData to Drive Quality Patient Care and Outcomes in 2014 http://t.co/2tdgA8pTis\n",
        "\n",
        "RT @varonis: What was Snowden REALLY up to?\n",
        "\n",
        "http://t.co/oxAwsi72qj\n",
        "\n",
        "#snowden #nsa #bigdata #privacy\n",
        "\n",
        "@JustJon @parallelbranch is offering free courses to everyone  at http://t.co/Y1DRH2EB1t \n",
        "#oracle #sharepoint #bigdata #hadoop\n",
        "\n",
        "RT @albertg2g: .@jfvillena es curioso, todo el mundo tiene su propia forma de hablar de #BigData pero el uso de la metafora \"piscina de dat\u2026\n",
        "\n",
        "Checklist for managing growing IT workloads in your #bigdata environment: http://t.co/5SoVigBGzP via @CIOonline #BMCControlM\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "from twitter import *\n",
      "import json\n",
      "\n",
      "OAUTH_TOKEN = '29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo'\n",
      "OAUTH_SECRET = 'dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds'\n",
      "CONSUMER_KEY = 'xLNSlDfsYDWoVDVlnUewog'\n",
      "CONSUMER_SECRET = 'cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc'\n",
      "\n",
      "# see \"Authentication\" section below for tokens and keys\n",
      "t = Twitter(\n",
      "            auth=OAuth(OAUTH_TOKEN, OAUTH_SECRET,\n",
      "                       CONSUMER_KEY, CONSUMER_SECRET)\n",
      "           )\n",
      "\n",
      "# Get your \"home\" timeline\n",
      "#t.statuses.home_timeline()\n",
      "query = \"#bigdata\"\n",
      "myCount = 10\n",
      "myStart = 1\n",
      "\n",
      "# twitter search\n",
      "r = t.search.tweets(q=query,count=myCount)\n",
      "tweets = json.loads(json.dumps(r['statuses'], indent=1))\n",
      "my_max_id = r['search_metadata']['max_id']\n",
      "print r['search_metadata']['next_results']\n",
      "\n",
      "\"\"\"\n",
      "#write to file for review\n",
      "tweets_data = 'C:/Projects/ipython/tweets.json'\n",
      "f = open(tweets_data, 'w')\n",
      "f.write(json.dumps(r, indent=2))\n",
      "f.close()\n",
      "\"\"\"\n",
      "\n",
      "for k in range(0,5): \n",
      "    try:\n",
      "        next_results = r['search_metadata']['next_results']\n",
      "        my_max_id2 = r['search_metadata']['max_id']\n",
      "        print next_results\n",
      "        \n",
      "    except KeyError, e: # No more results when next_results doesn't exist\n",
      "        break\n",
      "    \n",
      "    r = t.search.tweets(q=query,count=myCount,max_id=my_max_id2-1)\n",
      "    tweets += json.loads(json.dumps(r['statuses'], indent=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "?max_id=414097079624282111&q=%23bigdata&count=10&include_entities=1\n",
        "?max_id=414097079624282111&q=%23bigdata&count=10&include_entities=1\n",
        "?max_id=414097063723282431&q=%23bigdata&count=10&include_entities=1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "?max_id=414097063723282431&q=%23bigdata&count=10&include_entities=1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "?max_id=414097063723282431&q=%23bigdata&count=10&include_entities=1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "?max_id=414097063723282431&q=%23bigdata&count=10&include_entities=1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Capture Streaming Data using Tweepy"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "http://nbviewer.ipython.org/github/raynach/hse-twitter/blob/master/docs/Collecting%20Twitter%20data%20from%20the%20API%20with%20Python.ipynb\n",
      "\n",
      "http://www.alex-hanna.com/tworkshops/lesson-7-mention-network-analysis/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "from tweepy import StreamListener\n",
      "import json, time, sys\n",
      "\n",
      "class SListener(StreamListener):\n",
      "\n",
      "    def __init__(self, api = None, fprefix = 'streamer'):\n",
      "        self.api = api or API()\n",
      "        self.counter = 0\n",
      "        self.fprefix = fprefix\n",
      "        self.output  = open('C:/Projects/ipython/data/' + fprefix + '.' \n",
      "                            + time.strftime('%Y%m%d-%H%M%S') + '.json', 'w')\n",
      "        self.delout  = open('C:/Projects/ipython/data/delete.txt', 'a')\n",
      "\n",
      "    def on_data(self, data):\n",
      "\n",
      "        if  'in_reply_to_status' in data:\n",
      "            self.on_status(data)\n",
      "        elif 'delete' in data:\n",
      "            delete = json.loads(data)['delete']['status']\n",
      "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
      "                return False\n",
      "        elif 'limit' in data:\n",
      "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
      "                return False\n",
      "        elif 'warning' in data:\n",
      "            warning = json.loads(data)['warnings']\n",
      "            print warning['message']\n",
      "            return false\n",
      "\n",
      "    def on_status(self, status):\n",
      "        self.output.write(status + \"\\n\")\n",
      "\n",
      "        self.counter += 1\n",
      "\n",
      "        if self.counter >= 20000:\n",
      "            self.output.close()\n",
      "            self.output = open('C:/Projects/ipython/data/' + self.fprefix + '.' \n",
      "                               + time.strftime('%Y%m%d-%H%M%S') + '.json', 'w')\n",
      "            self.counter = 0\n",
      "\n",
      "        return\n",
      "\n",
      "    def on_delete(self, status_id, user_id):\n",
      "        self.delout.write( str(status_id) + \"\\n\")\n",
      "        return\n",
      "\n",
      "    def on_limit(self, track):\n",
      "        sys.stderr.write(track + \"\\n\")\n",
      "        return\n",
      "\n",
      "    def on_error(self, status_code):\n",
      "        sys.stderr.write('Error: ' + str(status_code) + \"\\n\")\n",
      "        return False\n",
      "\n",
      "    def on_timeout(self):\n",
      "        sys.stderr.write(\"Timeout, sleeping for 60 seconds...\\n\")\n",
      "        time.sleep(60)\n",
      "        return \n",
      "\n",
      "    \n",
      "    import time, tweepy, sys\n",
      "\n",
      "## auth. \n",
      "## TK: Edit the username and password fields to authenticate from Twitter.\n",
      "#username = ''\n",
      "#password = ''\n",
      "#auth     = tweepy.auth.BasicAuthHandler(username, password)\n",
      "\n",
      "\n",
      "## Eventually you'll need to use OAuth. Here's the code for it here.\n",
      "## You can learn more about OAuth here: https://dev.twitter.com/docs/auth/oauth\n",
      "#OAUTH_TOKEN = '29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo'\n",
      "#OAUTH_SECRET = 'dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds'\n",
      "#CONSUMER_KEY = 'xLNSlDfsYDWoVDVlnUewog'\n",
      "#CONSUMER_SECRET = 'cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc'\n",
      "consumer_key        = \"xLNSlDfsYDWoVDVlnUewog\"\n",
      "consumer_secret     = \"cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc\"\n",
      "access_token        = \"29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo\"\n",
      "access_token_secret = \"dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds\"\n",
      "\n",
      "# OAuth process, using the keys and tokens\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "def main( mode = 1 ):\n",
      "    # track is for terms to be tracked\n",
      "    # follow is a list of userid's that you want to get all of their statuses\n",
      "    # total volume of whatever you pull back is limited to 1% of the overall twitter volume at that moment\n",
      "    #\n",
      "    #track  = ['#bigdata','#analytics','#socbiz','#datamining','ValaAfshar','burkepowers','KirkDBorne','gwenthomasdgi','billfranksga','ConstellationRG','DeloitteBA','iianalytics','EdwardTufte','Doug_Laney','johnelder4','davidloshin','DanVesset','isaiahgoodall','usamaf','bevelson','mikeferguson1','Freakalytics','predictanalytic','howarddresner','ROIRevolution','ocdqblog','jakeporway','brunoaziza','btemkin','beyondthearc','drewconway','ariegoldshlager','deanabb','SethGrimes','PatrickMeier','jilldyche','Claudia_Imhoff','weckerson','fivethirtyeight','mitsmr','avinash','1to1Media','donalddotfarmer','AlanSee','rwang0','DataMiningBlog','mgualtieri','JeffJonas','tdav','jamet123','mcristia','jowyang','briansolis','@merv','ISpeakAnalytics','KenexaSocial','ted_friedman','databaseguru','steve_dine','IBMSPSS','marksmithvr','INFORMS','infomgmt','kdnuggets','jeffreyfkelly','bigdata','ventanaresearch','ColinJWhite','SmarterPlanet','SmartDataCo','dataqualitypro','wiseanalytics','ibmbizanalytics','analyticbridge','forrester']\n",
      "    track  = ['#bigdata','#analytics','#socbiz','#datamining']\n",
      "    follow = [534563976,14072398,15822273,108286674,20167623,485055264,18503214,39600478,14562685,339653183,14060372,192579441,135945569,39413322,309425789,38670441,250103509,107376471,23204397,10915042,19573968,7617702,16871339,16948477,97348240,20748873,119802433,62316970,23230622,250257799,5676402,10781952,16895951,1585,15257533,64368500,14457688,16689471,14847675,18068926,92116069,22059787,38427411,25136114,11581462,894057794,317005231,9526012,16534327,42079656,77449588,18638602,10565902,237413764,14133436,17817175,113022733,10748252,805323,259725229,23242773,33042238,16598272,15160685,14065040,14191961,16693331,803694,41788528,39804979,43878033,3709051,243465644,193030139,194081605,107938429,14184076,21393134,15481072,27549343,591248843,79275123,168282354,294405972,397450344,15612251,139169346,17850251,2981431,17204373,63875612,26412800,15298751,38790830,14746910,22238542,89440968,16688845,246346708,7480172,260909325,39345049,18318677,14093970,365453693,22631958,11344912,198483889,14065217,227423290,19825996,4921851,705287821,34155148,263830118,260582890,17043455,21922529,5727392,19123248,18914917,13860052,15344094,63109065,15374525,35186545,141752902,15106068,14596096,14994269,22773313,29649822,134908142,14761966,41381572,21566520,13455092,20453726,118030412,9965842,90478485,16816193,582199192,15336340,112717095,53257764,218155014,14066062,217488696,23114136]\n",
      "            \n",
      "    listen = SListener(api, 'test')\n",
      "    stream = tweepy.Stream(auth, listen)\n",
      "\n",
      "    print \"Streaming started on %s users and %s keywords...\" % (len(track), len(follow))\n",
      "\n",
      "    try: \n",
      "        stream.filter(track = track, follow = follow)\n",
      "        #stream.sample()\n",
      "    except:\n",
      "        print \"We had an unknown error! Script halted.\"\n",
      "        stream.disconnect()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Streaming started on 4 users and 150 keywords...\n",
        "We had an unknown error! Script halted."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pagination through user timeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "from tweepy import StreamListener\n",
      "import json, time, sys\n",
      "\n",
      "consumer_key        = \"xLNSlDfsYDWoVDVlnUewog\"\n",
      "consumer_secret     = \"cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc\"\n",
      "access_token        = \"29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo\"\n",
      "access_token_secret = \"dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds\"\n",
      "\n",
      "# OAuth process, using the keys and tokens\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "for page in tweepy.Cursor(api.user_timeline).pages(3):\n",
      "        for item in page:\n",
      "                print item.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @choccytracy: RT @ValaAfshar: Your brand is what people say about you when you're not in the room. The web is the room and it's social. \u2026\n",
        "RT @rwang0: \"In a startup the purpose of analytics is to iterate product/market fit before the money runs out\" @blake #sxswv2v #reachhacking\n",
        "MT @SethGrimes: 'Should we be replacing \u201cr\u201d with \u201ce\u201d, identifying Influencees and not Influencers?' asks @Marie_Wallace #socbiz\n",
        "RT @kdnuggets: World Top \"Data\" Scientists before \"Data Science\": Shannon, Tukey, Kolmogorov, Markov,Tufte, (Turing?) http://t.co/iZrpm7B7PU\n",
        "RT @HarvardBiz: Don't Make Decisions, Orchestrate Them http://t.co/Q13oH5f08c #analytics #decisionmgt\n",
        "RT @kdnuggets: Twitter analytics cheat sheet, tips, and tactics http://t.co/tuafjXNV6B\n",
        "\"Numbers have an important story to tell.  They rely on you to give them a clear and convincing voice.\" -Stephen Few #analytics #decisionmgt\n",
        "Do you model feedback loops in biz decisions? \"Arctic ice grows darker\" http://t.co/bsaj4Wl5y8 @newscientist #decisionmgt #analytics\n",
        "#Analytics finds the strike zone. \"Why productive people stick to their strike zone\" http://t.co/DYfdIIqmN7 @FastCompany #decisionmgt\n",
        "IBM brought together the latest and best thinkers on segmentation. http://t.co/E1vKbRdXe4 @ibmbigdata #cxo #analytics #socbiz\n",
        "\"Never confuse motion with action.\" -Benjamin Franklin #analytics #decisionmgt #socbiz\n",
        "RT @kirkdborne: From Academic PhD to #DataScientist: 5 Tips from @DouglasMason10 for Making the Transition. http://t.co/IGhWgG5FPP\n",
        "Actionable? \"Tesla Model S vs Toyota Prius Interests, Datamining Shows Differences\" http://t.co/IfdThayUjV @greenoptimistic #analytics\n",
        "Actionable? \"Tesla Model S vs Toyota Prius Interests, Datamining Shows Differences\" http://t.co/0tR14zyE1x @greenoptimistic #analytics\n",
        "Actionable? \"Tesla Model S vs Toyota Prius Interests, Datamining Shows Differences\" http://t.co/Y2E3ws0DQC @greenoptimistic #analytics\n",
        "Is there evidence of backlash against generosity in #sm? \"When Good Deeds Are Punished\" http://t.co/sWzpKAbjDy @freakonomics #socbiz\n",
        "Technical, but good - Incorporate analytics properly. \"Analytical Scorecard Development\" http://t.co/oIchvHOgEd @roopamu #analytics #socbiz\n",
        "A worthy goal. Do we understand the causal direction? \"The Impossible Quest for Happiness\" http://t.co/D6emn2cowT @writenoreen #analytics\n",
        "Wish they'd teach this in school. \"5 Tips to Crack an Analytics Interview\" http://t.co/7ktPVvVGHY @jigsawacademy #analytics\n",
        "\"Get the habit of analysis. In time, it will enable synthesis to become your habit of mind.\" -Frank Lloyd Wright #analytics\n",
        "\"A point of view can be a dangerous luxury when substituted for insight and understanding.\" - Marshall McLuhan #analytics #decisionmgt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\"The Big Brand Theory: How Maersk Grew Social Media Influence in the Shipping Industry\" http://t.co/6VWE5xEdMP @RicDragon #socbiz\n",
        "Decision context of any analytics \"A Practitioner Speaks: Analytics and Decision Management\" http://t.co/OCWRilBKEk @jamet123 #analytics\n",
        "What events do you prefer? \"30 Meetings on Analytics, Big Data, Data Mining\" http://t.co/7vsfsAnL4q @kdnuggets #analytics #bigdata #socbiz\n",
        "What non-tech ?'s do you ask? \"89 job interview questions for data scientists\" http://t.co/vCSx4Eoy0y @analyticbridge #analytics #bigdata\n",
        "Interesting BBC documentary \"The Age of Big Data - BBC Documentary\" http://t.co/mVGrlDfgcV  @kdnuggets #bigdata #analytics\n",
        "Good writeup explaining big data and analytics to my grandma \"Mining big data\" http://t.co/DFg65rf33N @presscitizen #bigdata #analytics\n",
        "IBM only one to get 5 stars \"10 Enterprise Predictive Analytics Platforms Compared\" http://t.co/xnhxYJQVy9 @kdnuggets @ibmbigdata #analytics\n",
        "Talented volunteers needed \"How Big Data Can Help the Developing World Beat Poverty\" http://t.co/NPPTNcZ1Gk @VanRijmenam #bigdata #analytics\n",
        "Can we do this with in meetings? \"Teacher forces student to do math to unlock phone\" http://t.co/ggi0mywNq9 @FryRsquared #mathfun\n",
        "Example of cavalier causality \"That causal inference came out of nowhere\" http://t.co/4W7LBCkm4p @simplystats #analytics\n",
        "Another way to predict crime before it happens \"While You're Away, the Burglars Will Play\" http://t.co/IdL6rmXpq9 @Beth_Schultz #analytics\n",
        "Who would have guessed the future of space travel \"Crowd-Funding a Mission To Jupiter's Moons\" http://t.co/sWZ6pW4ucp @slashdot #socbiz\n",
        "What do you do when you get non-intuitive results? \"Do Baby Girls Cause Divorce?\" http://t.co/H3MN0aKzKe @Freakonomics #analytics\n",
        "@TIBCOSpotfire Thanks for the RT. Keep up the good work that we all enjoy so much.\n",
        "@IBMbigdata Thanks for the RT. And thanks for the great information.\n",
        "How are you measuring success? \"What are the best tools to measure social media influence?\" http://t.co/ELgO5ZtWNZ @lilachbullock #socbiz\n",
        "Recruiting curiosity \"Building An Analytics Dream Team\" http://t.co/ACUVJkacaH @AllAnalytics @Beth_Schultz #analytics #socbiz\n",
        "Do you know your target audience this well? \"STUDY: Venture Capital Influencer Insights\" http://t.co/5t9otoVodq #ZenoVCstudy #socbiz\n",
        "Fail to plan...\"Predictive analytics programs marred by poor planning, flawed models\" http://t.co/2EIUidWof1 @BizAnalyticsTT #analytics\n",
        "Business needs to guide \"Path on predictive analytics tools needs light from business beacons\" http://t.co/JnK4VrZx1p #analytics #socbiz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Text analytics gets you even better actions \"How to Do a Social Media Competitive Analysis\" http://t.co/Jesr6plAJY @socialcade #socbiz\n",
        "RT @Beth_Schultz \"RT @metabrown312: Why IT Doesn\u2019t \u201cGet\u201d Analytics (and how to make the first move in the right direction) \u00a0\"...\n",
        "Proper analytics gets your message to your customer \"Information Overload from Social Media\" http://t.co/Z07bHRqRak #socbiz #analytics\n",
        "Unique aspects to big data issues \"Today's Big Data is Not Yesterday's Big Data\" http://t.co/C8uKsib8Kr @analyticbridge #bigdata\n",
        "Best thinking on segmentation from many authors. \"Solving for Segmentation: Is Big Data \"X\"?\" http://t.co/kvac8jd4g6 @ibmbigdata #analytics\n",
        "Great visualizations \"Winning in Pharmaceutical Emerging Markets with Analytics\" http://t.co/Pf6VayyrjT @TIBCOSpotfire #analytics\n",
        "Interesting new tools \"Introducing New LinkedIn Company Page Analytics\" http://t.co/t9w0AjEKX7 @linkedin #analytics #socbiz\n",
        "This really seems to capture #socbiz \"Book Review: 5 Takeaways from Youtility by Jay Baer\" http://t.co/BBO7fRsWzt @DebraWEllis\n",
        "I see these all the time \"Common Misconceptions on Automating Decisions\" http://t.co/vCiwiadsFU @jamet123 #decisionmgt #analytics\n",
        "Cross-channel optimization \"How to Run a Successful Multi-Platform Hashtag Campaign\" http://t.co/W56NPZydrE @WillAtSMF #socbiz\n",
        "3 key competencies for #socbiz success \"Are You on Board with Social Business?\" http://t.co/EzrxUvpF4P @MJRomeriDDC\n",
        "Finding systemic problems \"Data Mining for Underpayments Yields Millions for Health System\" http://t.co/RLm5iBK39G #analytics\n",
        "Add cust journey map. RT @dreckbaerfrau: Learn the tricks of reputation management to increase customer satisfaction  http://t.co/cCUqjjBgH3\n",
        "RT @sniperanalytics: \"Companies are better at collecting data about customers than they are at using it to service them\".(Who can afford...\n",
        "@data_nerd @deanabb So, for you, what were key 2)??? experiences? For me, it was learning how to present technical to non-technical folks.\n",
        "@data_nerd Agreed, Data Scientist aren't created in College. Path is 1) school 2) ??? 3) big data job 4) profit. Good #bigdata talent rare.\n",
        "@data_nerd I mentor &amp; train analytics. Many of the best come from EDW #bigdata backgrounds. Soft skills most often lacking &amp; hard to train.\n",
        "Always asking better questions. RT @YvesMulkers Big Data Observations: The Science of Asking Questions http://t.co/FKIV8UvLmZ #analytics\n",
        "RT @Doug_Laney RT @sve_sic: Key soft skills of #datascientist: communication, collaboration, leadership, creativity, passion &amp; discipline\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "from tweepy import StreamListener\n",
      "import json, time, sys\n",
      "\n",
      "consumer_key        = \"xLNSlDfsYDWoVDVlnUewog\"\n",
      "consumer_secret     = \"cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc\"\n",
      "access_token        = \"29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo\"\n",
      "access_token_secret = \"dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds\"\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "    auth.set_access_token(access_token, access_token_secret)\n",
      "    \n",
      "    api = tweepy.API([auth], # support for multiple authentication handlers\n",
      "                     retry_count=3, retry_delay=5, retry_errors=set([401, 404, 500, 503]), # retry 3 times with 5 seconds delay when getting these error codes. For more details see https://dev.twitter.com/docs/error-codes-responses\n",
      "                     monitor_rate_limit=True, wait_on_rate_limit=True # monitor remaining calls and block until replenished\n",
      "                    )\n",
      "    \n",
      "    query = 'cupcake OR donut'\n",
      "    page_count = 0\n",
      "    for tweets in tweepy.Cursor(api.search, q=query, count=100, result_type=\"recent\", include_entities=True).pages():\n",
      "        page_count += 1\n",
      "        # print just the first tweet out of every page of 100 tweets\n",
      "        print tweets[0].text.encode('utf-8')\n",
      "        # stop after retrieving 200 pages\n",
      "        if page_count >= 200:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Interesting links for predicting Twitter\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Twouija: http://tweet.tarikmoon.com/\n",
      "\n",
      "Summary for non-technical audience of features used in predicting twitter retweetability:\n",
      "http://www.adigaskell.org/blog/2012/01/24/10-steps-to-twitter-heaven/\n",
      "\n",
      "https://sites.google.com/site/twitterresearch09/articles/untitledpost-1\n",
      "https://sites.google.com/site/twitterresearch09/articles/theroleofopinionleadersininterestbasedcommunities/amodelofretweets\n",
      "\n",
      "http://www.bloomagency.co.uk/using-data-to-delight-and-excite/\n",
      "\n",
      "Research Links\n",
      "http://www.mendeley.com/research-papers/search/?query=tweet\n",
      "https://sites.google.com/site/twitterresearch09/twitter-papers"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Useful Snippets for Operationalizing"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Running Python as a Service in Windows\n",
      "http://stackoverflow.com/questions/32404/can-i-run-a-python-script-as-a-service-in-windows-how\n",
      "http://stackoverflow.com/questions/3783269/how-do-i-make-a-python-script-installed-as-a-service-survive-a-logout\n",
      "http://code.activestate.com/recipes/551780-win-services-helper/\n",
      "\n",
      "https://www.wakari.io/plans\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}