{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Notes on functionality still needed:\n",
      "- detect spikes in hashtags the group being monitored follows and add that to the tags monitored\n",
      "    - control chart logic/tracking of tags\n",
      "    - dynamically add and remove hashtags\n",
      "- send digest of alerts via email\n",
      "- web application front end to \n",
      "    - view channel content from each channel, \n",
      "    - view recommendations, \n",
      "    - schedule messages, \n",
      "    - approve automatically created messages, \n",
      "    - send messages to all channels\n",
      "    - track influence metrics (that will be defined later as our analysis reveals drivers)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import with_statement"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "This is the working script that reads a stream from Twitter"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The service script was causing complex problems, so I used this script and set it up as a chron job (task scheduler) that will check every minute to make sure the process is running.  If it is not running, then it will start it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "from tweepy import StreamListener\n",
      "import json, time, sys\n",
      "\n",
      "class SListener(StreamListener):\n",
      "\n",
      "    def __init__(self, api = None, fprefix = 'streamer'):\n",
      "        self.api = api or API()\n",
      "        self.counter = 0\n",
      "        self.fprefix = fprefix\n",
      "        self.output  = open('C:/Projects/ipython/data/' + fprefix + '.' \n",
      "                            + time.strftime('%Y%m%d-%H%M%S') + '.json', 'w')\n",
      "        self.delout  = open('C:/Projects/ipython/data/delete.txt', 'a')\n",
      "\n",
      "    def on_data(self, data):\n",
      "\n",
      "        if  'in_reply_to_status' in data:\n",
      "            self.on_status(data)\n",
      "        elif 'delete' in data:\n",
      "            delete = json.loads(data)['delete']['status']\n",
      "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
      "                return False\n",
      "        elif 'limit' in data:\n",
      "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
      "                return False\n",
      "        elif 'warning' in data:\n",
      "            warning = json.loads(data)['warnings']\n",
      "            print warning['message']\n",
      "            return false\n",
      "\n",
      "    def on_status(self, status):\n",
      "        self.output.write(status + \"\\n\")\n",
      "\n",
      "        self.counter += 1\n",
      "\n",
      "        if self.counter >= 20000:\n",
      "            self.output.close()\n",
      "            self.output = open('C:/Projects/ipython/data/' + self.fprefix + '.' \n",
      "                               + time.strftime('%Y%m%d-%H%M%S') + '.json', 'w')\n",
      "            self.counter = 0\n",
      "\n",
      "        return\n",
      "\n",
      "    def on_delete(self, status_id, user_id):\n",
      "        self.delout.write( str(status_id) + \"\\n\")\n",
      "        return\n",
      "\n",
      "    def on_limit(self, track):\n",
      "        sys.stderr.write(track + \"\\n\")\n",
      "        return\n",
      "\n",
      "    def on_error(self, status_code):\n",
      "        sys.stderr.write('Error: ' + str(status_code) + \"\\n\")\n",
      "        return False\n",
      "\n",
      "    def on_timeout(self):\n",
      "        sys.stderr.write(\"Timeout, sleeping for 60 seconds...\\n\")\n",
      "        time.sleep(60)\n",
      "        return \n",
      "\n",
      "    \n",
      "    import time, tweepy, sys\n",
      "\n",
      "## auth. \n",
      "## TK: Edit the username and password fields to authenticate from Twitter.\n",
      "#username = ''\n",
      "#password = ''\n",
      "#auth     = tweepy.auth.BasicAuthHandler(username, password)\n",
      "\n",
      "\n",
      "## Eventually you'll need to use OAuth. Here's the code for it here.\n",
      "## You can learn more about OAuth here: https://dev.twitter.com/docs/auth/oauth\n",
      "#OAUTH_TOKEN = '29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo'\n",
      "#OAUTH_SECRET = 'dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds'\n",
      "#CONSUMER_KEY = 'xLNSlDfsYDWoVDVlnUewog'\n",
      "#CONSUMER_SECRET = 'cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc'\n",
      "consumer_key        = \"xLNSlDfsYDWoVDVlnUewog\"\n",
      "consumer_secret     = \"cysEIjDiZ29J81XbMpzXndE0s7cAL5lX7XygY4G80Qc\"\n",
      "access_token        = \"29499583-TN805p7dPVvBnaBhU5ldegeQTx16JqYMaFM8AkzSo\"\n",
      "access_token_secret = \"dd4cSOXoruGfoykzlx6gHcSTIEQDuQkTsW4qobFyds\"\n",
      "\n",
      "# OAuth process, using the keys and tokens\n",
      "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
      "auth.set_access_token(access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "def main( mode = 1 ):\n",
      "    # track: list of keywords or terms to be tracked\n",
      "    # follow: list of userid's that you want to get all of their statuses\n",
      "    #\n",
      "    # note: total volume of whatever you pull back is limited to 1% of the overall twitter volume at that moment\n",
      "    #\n",
      "    #track  = ['#bigdata','#analytics','#socbiz','#datamining','ValaAfshar','burkepowers','KirkDBorne','gwenthomasdgi','billfranksga','ConstellationRG','DeloitteBA','iianalytics','EdwardTufte','Doug_Laney','johnelder4','davidloshin','DanVesset','isaiahgoodall','usamaf','bevelson','mikeferguson1','Freakalytics','predictanalytic','howarddresner','ROIRevolution','ocdqblog','jakeporway','brunoaziza','btemkin','beyondthearc','drewconway','ariegoldshlager','deanabb','SethGrimes','PatrickMeier','jilldyche','Claudia_Imhoff','weckerson','fivethirtyeight','mitsmr','avinash','1to1Media','donalddotfarmer','AlanSee','rwang0','DataMiningBlog','mgualtieri','JeffJonas','tdav','jamet123','mcristia','jowyang','briansolis','@merv','ISpeakAnalytics','KenexaSocial','ted_friedman','databaseguru','steve_dine','IBMSPSS','marksmithvr','INFORMS','infomgmt','kdnuggets','jeffreyfkelly','bigdata','ventanaresearch','ColinJWhite','SmarterPlanet','SmartDataCo','dataqualitypro','wiseanalytics','ibmbizanalytics','analyticbridge','forrester']\n",
      "    track  = ['#bigdata','#analytics','#socbiz','#datamining']\n",
      "    follow = [534563976,14072398,15822273,108286674,20167623,485055264,18503214,39600478,14562685,339653183,14060372,192579441,135945569,39413322,309425789,38670441,250103509,107376471,23204397,10915042,19573968,7617702,16871339,16948477,97348240,20748873,119802433,62316970,23230622,250257799,5676402,10781952,16895951,1585,15257533,64368500,14457688,16689471,14847675,18068926,92116069,22059787,38427411,25136114,11581462,894057794,317005231,9526012,16534327,42079656,77449588,18638602,10565902,237413764,14133436,17817175,113022733,10748252,805323,259725229,23242773,33042238,16598272,15160685,14065040,14191961,16693331,803694,41788528,39804979,43878033,3709051,243465644,193030139,194081605,107938429,14184076,21393134,15481072,27549343,591248843,79275123,168282354,294405972,397450344,15612251,139169346,17850251,2981431,17204373,63875612,26412800,15298751,38790830,14746910,22238542,89440968,16688845,246346708,7480172,260909325,39345049,18318677,14093970,365453693,22631958,11344912,198483889,14065217,227423290,19825996,4921851,705287821,34155148,263830118,260582890,17043455,21922529,5727392,19123248,18914917,13860052,15344094,63109065,15374525,35186545,141752902,15106068,14596096,14994269,22773313,29649822,134908142,14761966,41381572,21566520,13455092,20453726,118030412,9965842,90478485,16816193,582199192,15336340,112717095,53257764,218155014,14066062,217488696,23114136]\n",
      "            \n",
      "    listen = SListener(api, 'test')\n",
      "    stream = tweepy.Stream(auth, listen)\n",
      "\n",
      "    print \"Streaming started on %s users and %s keywords...\" % (len(track), len(follow))\n",
      "\n",
      "    try: \n",
      "        stream.filter(track = track, follow = follow)\n",
      "        #stream.sample()\n",
      "    except:\n",
      "        print \"We had an unknown error! Script halted.\"\n",
      "        stream.disconnect()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Merge JSON files of captured stream data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import shutil, glob, time\n",
      "\n",
      "outfilename = 'C:\\\\Projects\\\\ipython\\\\data\\\\twitterStreamSample' + time.strftime('%Y%m%d-%H%M%S') + '.json'\n",
      "\n",
      "with open(outfilename, 'wb') as outfile:\n",
      "    for filename in glob.glob('C:\\\\Projects\\\\ipython\\\\data\\\\*.json'):\n",
      "        if filename <> outfilename:\n",
      "            print filename # show progress\n",
      "            with open(filename) as readfile:            \n",
      "                shutil.copyfileobj(readfile, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Projects\\ipython\\data\\test.20140211-224451.json\n",
        "C:\\Projects\\ipython\\data\\test.20140211-235755.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140212-042455.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140212-083751.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140212-092451.json\n",
        "C:\\Projects\\ipython\\data\\test.20140212-135759.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140212-205832.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140213-001753.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140213-004651.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140213-005451.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140213-083651.json\n",
        "C:\\Projects\\ipython\\data\\test.20140213-182551.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-004253.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-040653.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-085851.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-174054.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-174251.json\n",
        "C:\\Projects\\ipython\\data\\test.20140214-174952.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140214-181251.json\n",
        "C:\\Projects\\ipython\\data\\test.20140214-181951.json\n",
        "C:\\Projects\\ipython\\data\\test.20140215-142451.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140215-145051.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140215-145951.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140215-150951.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140215-192754.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140215-223751.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-034051.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-061851.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-081451.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-082551.json\n",
        "C:\\Projects\\ipython\\data\\test.20140216-123851.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-142551.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-143651.json\n",
        "C:\\Projects\\ipython\\data\\test.20140216-192951.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-194551.json\n",
        "C:\\Projects\\ipython\\data\\test.20140216-200151.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-224753.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-225551.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-230251.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140216-233451.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140217-072712.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140217-103153.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140217-104351.json\n",
        "C:\\Projects\\ipython\\data\\test.20140217-105051.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140217-105751.json\n",
        "C:\\Projects\\ipython\\data\\test.20140217-110251.json\n",
        "C:\\Projects\\ipython\\data\\test.20140217-110751.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140217-233053.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140218-001651.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140218-040053.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\test.20140218-190102.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C:\\Projects\\ipython\\data\\twitterStreamSample20140211-223539.json"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Convert JSON file to CSV for Modeler"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import csv\n",
      "\n",
      "#stem = 'twitterStreamSample20140207-124427'\n",
      "stem = 'twitterStreamSample20140219-213720'\n",
      "filename = 'C:/Projects/ipython/data/' + stem + '.json'\n",
      "dest_filename = 'C:/Projects/ipython/data/' + stem + '.csv'\n",
      "\n",
      "#ifile  = open(filename, \"rb\")\n",
      "#tweets = csv.reader(ifile)\n",
      "ofile  = open(dest_filename, \"wb\")\n",
      "writer = csv.writer(ofile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
      "\n",
      "    \n",
      "# header\n",
      "header = []\n",
      "header.append('TweetID')\n",
      "header.append('TweetDateTime')\n",
      "header.append('Author_ID')\n",
      "header.append('TweetText')\n",
      "header.append('RetweetCount')\n",
      "header.append('FavoriteCount')\n",
      "header.append('UserMentions')\n",
      "header.append('Hashtags')\n",
      "header.append('URLs')\n",
      "header.append('AuthorName')\n",
      "header.append('FollowersCount')\n",
      "header.append('FriendsCount')\n",
      "header.append('StatusesCount')\n",
      "header.append('UserLocation')\n",
      "header.append('Description')\n",
      "header.append('TwitterHandle')\n",
      "\n",
      "writer.writerow(header)\n",
      "\n",
      "with open(filename) as tweets:\n",
      "    #data = json.load(tweets)\n",
      "    #print type(data)\n",
      "    \n",
      "    for line in tweets:\n",
      "        row = []\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "        except ValueError as detail:\n",
      "            continue    \n",
      "        \n",
      "        if tweet['id']: #'TweetID'\n",
      "            row.append(tweet['id'])\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "        \n",
      "        if tweet['created_at']: #'TweetDateTime'\n",
      "            row.append(tweet['created_at'])\n",
      "        else:\n",
      "            row.append(\"\")        \n",
      "        \n",
      "        if tweet['user']['id']: #'Author_ID'\n",
      "            row.append(tweet['user']['id']) \n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['text']: #'TweetText'\n",
      "            row.append(tweet['text'].encode('utf-8').replace(\"\\n\", \" \").replace(\"\\r\", \"\"))\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if 'retweeted_status' in tweet: #'RetweetCount'\n",
      "            row.append(tweet['retweeted_status']['retweet_count'])\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if 'retweeted_status' in tweet: #'FavoriteCount'\n",
      "            row.append(tweet['retweeted_status']['favorite_count'])\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "           \n",
      "        if tweet['entities']: #'UserMentions'\n",
      "            mentions = \"\"\n",
      "            if 'user_mentions' in tweet['entities']:                \n",
      "                for j in range(0,len(tweet['entities']['user_mentions'])):\n",
      "                    mentions = mentions + tweet['entities']['user_mentions'][j]['screen_name'].encode('utf-8') + \"|\"\n",
      "            row.append(mentions)\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "             \n",
      "        if tweet['entities']: #'Hashtags'\n",
      "            hashtags = \"\"\n",
      "            if 'hashtags' in tweet['entities']: \n",
      "                for j in range(0,len(tweet['entities']['hashtags'])):\n",
      "                    hashtags = hashtags + tweet['entities']['hashtags'][j]['text'].encode('utf-8') + \"|\"\n",
      "            row.append(hashtags)\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['entities']: #'URLs'\n",
      "            urls = \"\"\n",
      "            if 'urls' in tweet['entities']:                \n",
      "                for j in range(0,len(tweet['entities']['urls'])):\n",
      "                    urls = urls + tweet['entities']['urls'][j]['expanded_url'].encode('utf-8') + \"|\"\n",
      "            row.append(urls) \n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['name']: #'AuthorName'\n",
      "            row.append(tweet['user']['name'].encode('utf-8'))\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['followers_count']: #'FollowersCount'\n",
      "            row.append(tweet['user']['followers_count']) \n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['friends_count']: #'FriendsCount'\n",
      "            row.append(tweet['user']['friends_count']) \n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['statuses_count']: #'StatusesCount'\n",
      "            row.append(tweet['user']['statuses_count'])\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['location']: #'UserLocation'\n",
      "            row.append(tweet['user']['location'].encode('utf-8'))\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "            \n",
      "        if tweet['user']['description']: #'Description'\n",
      "            row.append(tweet['user']['description'].encode('utf-8').replace(\"\\n\", \" \").replace(\"\\r\", \"\"))\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "        \n",
      "        if tweet['user']['screen_name']: #'AuthorHandle'\n",
      "            row.append(tweet['user']['screen_name'].encode('utf-8'))\n",
      "        else:\n",
      "            row.append(\"\")\n",
      "\n",
      "        writer.writerow(row)\n",
      "        \n",
      "ofile.close()\n",
      "#ifile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Convert merged JSON file to Excel for modeler"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "from openpyxl import Workbook\n",
      "\n",
      "filename = 'C:/Projects/ipython/data/twitterStreamSample20140126-212358.json'\n",
      "\n",
      "with open(filename, 'rb') as tweets:     \n",
      "    wb = Workbook()\n",
      "    ws = wb.get_active_sheet()\n",
      "    \n",
      "    dest_filename = 'C:/Projects/ipython/data/twitterStreamSample20140126-212358.xls'\n",
      "    \n",
      "    ws = wb.worksheets[0]    \n",
      "    ws.title = \"Twitter\"\n",
      "    \n",
      "    # create header\n",
      "    ws.cell('A1').value = 'TweetID'\n",
      "    ws.cell('B1').value = 'TweetDateTime'\n",
      "    ws.cell('C1').value = 'Author_ID'\n",
      "    ws.cell('D1').value = 'TweetText'\n",
      "    ws.cell('E1').value = 'RetweetCount'\n",
      "    ws.cell('F1').value = 'FavoriteCount'\n",
      "    ws.cell('G1').value = 'UserMentions'\n",
      "    ws.cell('H1').value = 'Hashtags'\n",
      "    ws.cell('I1').value = 'URLs'\n",
      "    ws.cell('J1').value = 'AuthorName'\n",
      "    ws.cell('K1').value = 'FollowersCount'\n",
      "    ws.cell('L1').value = 'FriendsCount'\n",
      "    ws.cell('M1').value = 'StatusesCount'\n",
      "    ws.cell('N1').value = 'UserLocation'\n",
      "    ws.cell('O1').value = 'Description'\n",
      "    ws.cell('P1').value = 'TwitterHandle'\n",
      "    \n",
      "    i = 2\n",
      "    for line in tweets:\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "        except ValueError as detail:\n",
      "            continue\n",
      "        if tweet['id']: #'TweetID'\n",
      "            ws.cell('%s%s'%('A', i)).value = tweet['id']\n",
      "        else:\n",
      "            ws.cell('%s%s'%('A', i)).value = \"\" \n",
      "        \n",
      "        if tweet['created_at']: #'TweetDateTime'\n",
      "            ws.cell('%s%s'%('B', i)).value = ['tweet.created_at']\n",
      "        else:\n",
      "            ws.cell('%s%s'%('B', i)).value = \"\"        \n",
      "        \n",
      "        if tweet['user']['id']: #'Author_ID'\n",
      "            ws.cell('%s%s'%('C', i)).value = tweet['user']['id'] \n",
      "        else:\n",
      "            ws.cell('%s%s'%('C', i)).value = \"\"\n",
      "            \n",
      "        if tweet['text']: #'TweetText'\n",
      "            ws.cell('%s%s'%('D', i)).value = tweet['text'].encode('utf-8') \n",
      "        else:\n",
      "            ws.cell('%s%s'%('D', i)).value = \"\"\n",
      "            \n",
      "        if tweet['retweet_count']: #'RetweetCount'\n",
      "            ws.cell('%s%s'%('E', i)).value = tweet['retweet_count']\n",
      "        else:\n",
      "            ws.cell('%s%s'%('E', i)).value = \"\"\n",
      "            \n",
      "        if tweet['favorite_count']: #'FavoriteCount'\n",
      "            ws.cell('%s%s'%('F', i)).value = tweet['favorite_count']\n",
      "        else:\n",
      "            ws.cell('%s%s'%('F', i)).value = \"\"\n",
      "           \n",
      "        if tweet['entities']: #'UserMentions'\n",
      "            mentions = \"\"\n",
      "            if 'user_mentions' in tweet['entities']:                \n",
      "                for j in range(0,len(tweet['entities']['user_mentions'])):\n",
      "                    mentions = mentions + tweet['entities']['user_mentions'][j]['screen_name'].encode('utf-8') + \"|\"\n",
      "            ws.cell('%s%s'%('G', i)).value = mentions\n",
      "        else:\n",
      "            ws.cell('%s%s'%('G', i)).value = \"\"\n",
      "             \n",
      "        if tweet['entities']: #'Hashtags'\n",
      "            hashtags = \"\"\n",
      "            if 'hashtags' in tweet['entities']: \n",
      "                for j in range(0,len(tweet['entities']['hashtags'])):\n",
      "                    hashtags = hashtags + tweet['entities']['hashtags'][j]['text'].encode('utf-8') + \"|\"\n",
      "            ws.cell('%s%s'%('H', i)).value = hashtags\n",
      "        else:\n",
      "            ws.cell('%s%s'%('H', i)).value = \"\"\n",
      "            \n",
      "        if tweet['entities']: #'URLs'\n",
      "            urls = \"\"\n",
      "            if 'urls' in tweet['entities']:                \n",
      "                for j in range(0,len(tweet['entities']['urls'])):\n",
      "                    urls = urls + tweet['entities']['urls'][j]['expanded_url'].encode('utf-8') + \"|\"\n",
      "            ws.cell('%s%s'%('I', i)).value = urls \n",
      "        else:\n",
      "            ws.cell('%s%s'%('I', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['name']: #'AuthorName'\n",
      "            ws.cell('%s%s'%('J', i)).value = tweet['user']['name'].encode('utf-8')\n",
      "        else:\n",
      "            ws.cell('%s%s'%('J', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['followers_count']: #'FollowersCount'\n",
      "            ws.cell('%s%s'%('K', i)).value = tweet['user']['followers_count'] \n",
      "        else:\n",
      "            ws.cell('%s%s'%('K', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['friends_count']: #'FriendsCount'\n",
      "            ws.cell('%s%s'%('L', i)).value = tweet['user']['friends_count'] \n",
      "        else:\n",
      "            ws.cell('%s%s'%('L', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['statuses_count']: #'StatusesCount'\n",
      "            ws.cell('%s%s'%('M', i)).value = tweet['user']['statuses_count']\n",
      "        else:\n",
      "            ws.cell('%s%s'%('M', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['location']: #'UserLocation'\n",
      "            ws.cell('%s%s'%('N', i)).value = tweet['user']['location'].encode('utf-8')\n",
      "        else:\n",
      "            ws.cell('%s%s'%('N', i)).value = \"\"\n",
      "            \n",
      "        if tweet['user']['description']: #'Description'\n",
      "            ws.cell('%s%s'%('O', i)).value = tweet['user']['description'].encode('utf-8') \n",
      "        else:\n",
      "            ws.cell('%s%s'%('O', i)).value = \"\"\n",
      "        \n",
      "        if tweet['user']['screen_name']: #'AuthorHandle'\n",
      "            ws.cell('%s%s'%('P', i)).value = tweet['user']['screen_name'].encode('utf-8')\n",
      "        else:\n",
      "            ws.cell('%s%s'%('P', i)).value = \"\"\n",
      "            \n",
      "        i += 1\n",
      "       \n",
      "    wb.save(filename = dest_filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}